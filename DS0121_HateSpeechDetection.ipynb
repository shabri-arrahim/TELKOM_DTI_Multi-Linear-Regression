{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS0121_HateSpeechDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPGiDo1ui8evuHD/vruhuVK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shabri-arrahim/TELKOM_DTI_Multi-Linear-Regression/blob/master/DS0121_HateSpeechDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VipTN6_SRZyH"
      },
      "source": [
        "# Hate Speech Detection [Text Mining]\n",
        "\n",
        "Berikut merupakan implementasi dari $Text Mining$ untuk melakukan proses Hate Speech Detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQNJJMThS4lr"
      },
      "source": [
        "---\n",
        "---\n",
        "# [Part 1] Import Libraries and Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7XvWMMDS9ow"
      },
      "source": [
        "---\n",
        "## 1 - Import Libraries\n",
        "\n",
        "Import requiered libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKXHiTvRPe2R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f7a5ed-e572-45cf-88d4-7025c13d664f"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import string \n",
        "import random\n",
        "import pandas as pd\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfAcdgDs_F3r",
        "outputId": "55f5f33f-ad5d-4618-a9eb-fc61dba75232"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.cuda.FloatTensor\n",
        "\n",
        "## Uncomment out the following line if you're on a machine with a CPU set up for PyTorch!\n",
        "# dtype = torch.FloatTensor \n",
        "\n",
        "print(\"Runtime device: {}\\nTensor type: {}\".format(device, dtype))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime device: cuda\n",
            "Tensor type: <class 'torch.cuda.FloatTensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvTmxCvT4Dxq"
      },
      "source": [
        "---\n",
        "## 2 - Load IDHSD Data\n",
        "\n",
        "The dataset is a two columns data of: label - tweet, consist of 713 tweets in Indonesian.\n",
        "The label is Non_HS or HS. Non_HS for \"non-hate-speech\" tweet and HS for \"hate-speech\" tweet.\n",
        "\n",
        "*   Number of Non_HS tweets: 453\n",
        "*   Number of HS tweets: 260\n",
        "\n",
        "Since this dataset is unbalanced, you might have to do over-sampling/down-sampling in order to create a balanced dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikL7NnQl8hg6",
        "outputId": "9fcbf31e-f604-44d7-ff1f-715f15844110"
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/shabri-arrahim/TELKOM_DTI_Multi-Linear-Regression/master/datasets/IDHSD_RIO_unbalanced_713_2017.txt'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-27 17:15:23--  https://raw.githubusercontent.com/shabri-arrahim/TELKOM_DTI_Multi-Linear-Regression/master/datasets/IDHSD_RIO_unbalanced_713_2017.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77009 (75K) [text/plain]\n",
            "Saving to: ‘IDHSD_RIO_unbalanced_713_2017.txt.3’\n",
            "\n",
            "\r          IDHSD_RIO   0%[                    ]       0  --.-KB/s               \rIDHSD_RIO_unbalance 100%[===================>]  75.20K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2020-11-27 17:15:24 (10.8 MB/s) - ‘IDHSD_RIO_unbalanced_713_2017.txt.3’ saved [77009/77009]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "U6kiz5zd1lyI",
        "outputId": "c3febe51-e98d-4084-864b-0c7ada7f7b8f"
      },
      "source": [
        "with open('IDHSD_RIO_unbalanced_713_2017.txt', encoding=\"utf8\", errors=\"ignore\") as f:\n",
        "  df = pd.read_csv(f, sep='\\t')\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non_HS</td>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Label                                              Tweet\n",
              "0  Non_HS  RT @spardaxyz: Fadli Zon Minta Mendagri Segera...\n",
              "1  Non_HS  RT @baguscondromowo: Mereka terus melukai aksi...\n",
              "2  Non_HS  Sylvi: bagaimana gurbernur melakukan kekerasan...\n",
              "3  Non_HS  Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...\n",
              "4  Non_HS  RT @lisdaulay28: Waspada KTP palsu.....kawal P..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT2DCTrI8GPJ",
        "outputId": "474d21ba-68c2-4f75-94f1-238019e02623"
      },
      "source": [
        "features = df[['Tweet']]\n",
        "labels = df[['Label']]\n",
        "print(\"Feature shape: {}\\nLabel shape: {}\".format(features.shape, labels.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature shape: (713, 1)\n",
            "Label shape: (713, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "17e07Mj8h2ea",
        "outputId": "cf874846-c7c9-4b8c-c810-b41f8bd9bab4"
      },
      "source": [
        "features.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RT @spardaxyz: Fadli Zon Minta Mendagri Segera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RT @baguscondromowo: Mereka terus melukai aksi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sylvi: bagaimana gurbernur melakukan kekerasan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @lisdaulay28: Waspada KTP palsu.....kawal P...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet\n",
              "0  RT @spardaxyz: Fadli Zon Minta Mendagri Segera...\n",
              "1  RT @baguscondromowo: Mereka terus melukai aksi...\n",
              "2  Sylvi: bagaimana gurbernur melakukan kekerasan...\n",
              "3  Ahmad Dhani Tak Puas Debat Pilkada, Masalah Ja...\n",
              "4  RT @lisdaulay28: Waspada KTP palsu.....kawal P..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uqMqUePAbUh"
      },
      "source": [
        "---\n",
        "---\n",
        "# [Part 2] Pre-Processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLue_Q9alEwG"
      },
      "source": [
        "---\n",
        "## 1 - Data Cleaning (Text Cleaning, Tokenization, Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaVEds-CciH9"
      },
      "source": [
        "def text_clening(text):\n",
        "\n",
        "    # Create stemmer\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "\n",
        "    # Create Tokenizer\n",
        "    tok = WordPunctTokenizer()\n",
        "\n",
        "    # Create list of stopwords\n",
        "    stopwords = nltk.corpus.stopwords.words('indonesian')\n",
        "\n",
        "    # Text Cleaning Process\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    try:\n",
        "        text = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        text = souped\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    clean_text = emoji_pattern.sub(r'', text)\n",
        "    clean_text = ' '.join(re.sub(\"([RT])|(#[A-Za-z0-9]+)|(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(https?:[^ ]+)|(www.[^ ])\",\" \", text).split()).lower()\n",
        "    clean_text = ' '.join([word for word in clean_text.split() if word not in stopwords])\n",
        "\n",
        "    # Text tokenization and remove stopword\n",
        "    text_token = (\" \".join([x for x in tok.tokenize(clean_text) if len(x) > 1])).strip()\n",
        "\n",
        "    # Text stemming\n",
        "    text_stem = (\" \".join([stemmer.stem(x) for x in text_token.split(\" \")])).strip()\n",
        "\n",
        "    return text_stem"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GJXKq3WdCTk"
      },
      "source": [
        "clean_text = []\n",
        "for word in features.Tweet:\n",
        "  clean_text.append(text_clening(word))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I63yrZum6Hei"
      },
      "source": [
        "clean_features = pd.DataFrame({'tweet': clean_text})"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmNYTte76TVb"
      },
      "source": [
        "enc_labels = labels['Label'].map({'HS': 1, 'Non_HS': 0}))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOSyaQ2o5ULr"
      },
      "source": [
        "---\n",
        "## 2 - Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFDi1hNIqbOE"
      },
      "source": [
        "# TFIDF Class\n",
        "class TFIDF(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit_trasnform(self, data):\n",
        "    bookOfWord = self.__bookOfWord(data)\n",
        "    uniqueWords = self.__uniqueWords(data)\n",
        "    N_DOC = bookOfWord.shape[0]\n",
        "    IDF_DICT = dict.fromkeys(uniqueWords, 0)\n",
        "    TFIDF_LIST = []\n",
        "    for low in bookOfWord.to_list():\n",
        "      NOW = self.__numOfWord(low, uniqueWords)\n",
        "      TF = self.__computeTF(NOW, low)\n",
        "      IDF = self.__computeIDF(NOW, N_DOC, IDF_DICT)\n",
        "      TFIDF_LIST.append(self.__compute_TFIDF(TF, IDF))\n",
        "    return TFIDF_LIST\n",
        "\n",
        "  def __uniqueWords(self, data):\n",
        "    unique_words = set(list(data.str.split(' ', expand=True).stack().unique()))\n",
        "    if '' in unique_words:\n",
        "      unique_words.remove('')\n",
        "    return unique_words\n",
        "\n",
        "  def __bookOfWord(self, data):\n",
        "    bow = data.str.split()\n",
        "    return bow\n",
        "\n",
        "  #Compute NumOfWord\n",
        "  def __numOfWord(self, bookOfWord, uniqueWords):\n",
        "    numOfWord = dict.fromkeys(uniqueWords, 0)\n",
        "    for word in bookOfWord:\n",
        "      numOfWord[word] += 1\n",
        "    return numOfWord\n",
        "\n",
        "  #Compute TF\n",
        "  def __computeTF(self, wordOfDict, bagOfWord):\n",
        "    bowCount = len(bagOfWord)\n",
        "    tfDict = dict.fromkeys(wordOfDict.keys(), 0)\n",
        "    for word, count in wordOfDict.items(): \n",
        "      tfDict[word] = count / float(bowCount)\n",
        "    return tfDict\n",
        "\n",
        "  # Compute IDF\n",
        "  def __computeIDF(self, document, n_doc, idfDict):\n",
        "    for word, val in document.items():\n",
        "      if val > 0:\n",
        "        idfDict[word] += 1\n",
        "    for word, val in idfDict.items():\n",
        "      if val > 0:\n",
        "        idfDict[word] = np.log(n_doc+1/float(val)+1)+1\n",
        "    return idfDict\n",
        "\n",
        "  # Compute TFIDF\n",
        "  def __compute_TFIDF(self, listOftfBagOfWords, idfs):\n",
        "    tfidf = dict.fromkeys(listOftfBagOfWords.keys(), 0)\n",
        "    for word, val in listOftfBagOfWords.items():\n",
        "      tfidf[word] = val * idfs[word]\n",
        "    return tfidf"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJBx9tJn8QR8"
      },
      "source": [
        "vectorizer = TFIDF()\n",
        "features_vect = pd.DataFrame(vectorizer.fit_trasnform(clean_features['tweet']))"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "siwDPLyC8w_F",
        "outputId": "7b7ebd98-5a1e-48a0-fcd1-c6d51182d1e1"
      },
      "source": [
        "features_vect.head()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lau</th>\n",
              "      <th>masjid2</th>\n",
              "      <th>ajah</th>\n",
              "      <th>cara2</th>\n",
              "      <th>wagub</th>\n",
              "      <th>duh</th>\n",
              "      <th>on</th>\n",
              "      <th>emak</th>\n",
              "      <th>kristen</th>\n",
              "      <th>media</th>\n",
              "      <th>ho</th>\n",
              "      <th>ahay</th>\n",
              "      <th>cuihhhhhh</th>\n",
              "      <th>mimpi</th>\n",
              "      <th>kondisi</th>\n",
              "      <th>tancap</th>\n",
              "      <th>maidah</th>\n",
              "      <th>ngumpet2</th>\n",
              "      <th>skema</th>\n",
              "      <th>sandang</th>\n",
              "      <th>bohong</th>\n",
              "      <th>bongkar</th>\n",
              "      <th>rena</th>\n",
              "      <th>maju</th>\n",
              "      <th>akh</th>\n",
              "      <th>ira</th>\n",
              "      <th>pantes</th>\n",
              "      <th>tahu</th>\n",
              "      <th>share</th>\n",
              "      <th>germo</th>\n",
              "      <th>tuips</th>\n",
              "      <th>onde</th>\n",
              "      <th>ketutunan</th>\n",
              "      <th>up</th>\n",
              "      <th>dicopo</th>\n",
              "      <th>gusur</th>\n",
              "      <th>aktif</th>\n",
              "      <th>ge</th>\n",
              "      <th>asu</th>\n",
              "      <th>waduk</th>\n",
              "      <th>...</th>\n",
              "      <th>mang</th>\n",
              "      <th>familiar</th>\n",
              "      <th>kecu</th>\n",
              "      <th>sumber</th>\n",
              "      <th>daging</th>\n",
              "      <th>abai</th>\n",
              "      <th>cnn</th>\n",
              "      <th>resmi</th>\n",
              "      <th>putra</th>\n",
              "      <th>pokok</th>\n",
              "      <th>kaga</th>\n",
              "      <th>galak</th>\n",
              "      <th>100</th>\n",
              "      <th>setia</th>\n",
              "      <th>klo</th>\n",
              "      <th>cebong</th>\n",
              "      <th>jamban</th>\n",
              "      <th>nyerang</th>\n",
              "      <th>allah</th>\n",
              "      <th>kualitas</th>\n",
              "      <th>coklat</th>\n",
              "      <th>some</th>\n",
              "      <th>puluh</th>\n",
              "      <th>ujar</th>\n",
              "      <th>kali</th>\n",
              "      <th>patung</th>\n",
              "      <th>islamophobia</th>\n",
              "      <th>gub</th>\n",
              "      <th>uips</th>\n",
              "      <th>figur</th>\n",
              "      <th>lunak</th>\n",
              "      <th>atur</th>\n",
              "      <th>enak</th>\n",
              "      <th>mirip2</th>\n",
              "      <th>menang</th>\n",
              "      <th>tunda</th>\n",
              "      <th>xoxo</th>\n",
              "      <th>metode</th>\n",
              "      <th>seba</th>\n",
              "      <th>unfollow</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2257 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   lau  masjid2  ajah  cara2  wagub  ...  tunda  xoxo  metode  seba  unfollow\n",
              "0  0.0      0.0   0.0    0.0    0.0  ...    0.0   0.0     0.0   0.0       0.0\n",
              "1  0.0      0.0   0.0    0.0    0.0  ...    0.0   0.0     0.0   0.0       0.0\n",
              "2  0.0      0.0   0.0    0.0    0.0  ...    0.0   0.0     0.0   0.0       0.0\n",
              "3  0.0      0.0   0.0    0.0    0.0  ...    0.0   0.0     0.0   0.0       0.0\n",
              "4  0.0      0.0   0.0    0.0    0.0  ...    0.0   0.0     0.0   0.0       0.0\n",
              "\n",
              "[5 rows x 2257 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V66c38V9_iNa"
      },
      "source": [
        "---\n",
        "## 3 - Split Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWf-pMrl_Pz1",
        "outputId": "b0d42dda-917c-441a-8f4b-104b057e9d40"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features_vect.values, enc_labels.values, test_size=0.2, random_state=35)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(570, 2257)\n",
            "(570,)\n",
            "(143, 2257)\n",
            "(143,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJtSlQl5A8Jn"
      },
      "source": [
        "---\n",
        "---\n",
        "# [Part 3] Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly-ICd6FCoAY"
      },
      "source": [
        "---\n",
        "## 1 - Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2KbYP-dANP5",
        "outputId": "35ce548a-175a-4567-c43c-3f6f6c35677d"
      },
      "source": [
        "model = GaussianNB().fit(X_train, y_train)\n",
        "labels_pred = model.predict(X_test)\n",
        "labels_pred"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
              "       1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53sl9ZUfCtOm"
      },
      "source": [
        "---\n",
        "## 2 - Model Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMpX6KoABYuo",
        "outputId": "68a790fc-3009-45c1-d15c-9d2b23f78491"
      },
      "source": [
        "conf_mat = metrics.confusion_matrix(y_test, labels_pred)\n",
        "conf_mat"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[73, 28],\n",
              "       [11, 31]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQLAQLVgDO9D",
        "outputId": "03a5c42f-acc1-4d18-bc45-ce3c67e580c6"
      },
      "source": [
        "print('Test model accuracy: ', metrics.accuracy_score(y_test, labels_pred))\n",
        "print('Test model precision: ', metrics.precision_score(y_test, labels_pred))\n",
        "print('Test model recall: ', metrics.recall_score(y_test, labels_pred))\n",
        "print('Test model F1 Score: ', metrics.f1_score(y_test, labels_pred))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test model accuracy:  0.7272727272727273\n",
            "Test model precision:  0.5254237288135594\n",
            "Test model recall:  0.7380952380952381\n",
            "Test model F1 Score:  0.613861386138614\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}